{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:plasticity: True\n",
      "INFO:tensorflow:mode: pretrain\n",
      "INFO:tensorflow:traing on dataset: 1\n",
      "INFO:tensorflow:use train: 250\n",
      "INFO:tensorflow:from stack: 0\n",
      "INFO:tensorflow:model save dir: default\n",
      "INFO:tensorflow:eval step: 1\n",
      "INFO:tensorflow:batch size: 16\n",
      "Loading training data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../divided_cifar10_data\\\\data1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f1cdbc78cec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unknown mode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m     \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransfer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-f1cdbc78cec8>\u001b[0m in \u001b[0;36minference\u001b[1;34m(transfer)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------------------#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     train_dataset = data.train_input_fn(classcount=classcount, use_train=FLAGS.use_train, datapath=cifar10_path,\n\u001b[1;32m--> 122\u001b[1;33m                                         dataset=FLAGS.dataset, batchsize=FLAGS.batchsize, maxepochs=maxepochs)\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mtrain_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\git\\Plasticity\\data.py\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[1;34m(classcount, use_train, dataset, datapath, batchsize, maxepochs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasscount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_preprocessing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\git\\Plasticity\\data.py\u001b[0m in \u001b[0;36mtrain_data_loader\u001b[1;34m(dataset, datapath)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mdatafile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data1.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mlabelfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label1.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minput_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../divided_cifar10_data\\\\data1.npy'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# plasticity\n",
    "\n",
    "from __future__ import absolute_import \n",
    "from __future__ import division \n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import pickle \n",
    "import os \n",
    "import calendar\n",
    "import time\n",
    "import argparse \n",
    "import sys \n",
    "import plas_resnet\n",
    "import data\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "cifar10_path = '../divided_cifar10_data'\n",
    "model_root = '../tz1303/ckpts_5v5_plas'\n",
    "\n",
    "#pretrained_model = '/scratch/tz1303/ckpts_5v5_plas/1537256622/pre_plas_res20-8500'\n",
    "\n",
    "num_res_blocks = 3\n",
    "\n",
    "#batchsize = 32\n",
    "maxepochs = 100\n",
    "\n",
    "_HEIGHT = 32 \n",
    "_WIDTH = 32 \n",
    "_NUM_CHANNELS = 3\n",
    "\n",
    "classcount = 5 \n",
    "\n",
    "use_val = 15\n",
    "\n",
    "def make_model_dir():\n",
    "    if not FLAGS.save_dir == 'default':\n",
    "        model_dir = os.path.join(model_root, FLAGS.save_dir)\n",
    "    else:\n",
    "        timestamp = calendar.timegm(time.gmtime())\n",
    "        model_dir = os.path.join(model_root, str(timestamp))\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    tf.logging.info(model_dir)\n",
    "    return model_dir\n",
    "\n",
    "def train_layers(min_stack=0, first_min_block=0):\n",
    "    train_scope = []\n",
    "    \n",
    "    min_block = first_min_block\n",
    "    for stack in range(min_stack, 3):\n",
    "        for res_block in range(min_block, num_res_blocks):\n",
    "            train_scope.append(\"feature_extractor/{}_{}_one/\".format(stack, res_block))\n",
    "\n",
    "            train_scope.append(\"feature_extractor/{}_{}_two/\".format(stack, res_block))\n",
    "            if stack>0 and res_block == 0:\n",
    "                train_scope.append(\"feature_extractor/{}_{}_three/\".format(stack, res_block))\n",
    "        min_block = 0      \n",
    "\n",
    "    var = []\n",
    "    if min_stack == 0:\n",
    "        var.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'feature_extractor/first'))\n",
    "    for scope in train_scope:\n",
    "        var.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n",
    "    var.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'classifier'))\n",
    "    train_var = [item for sublist in var for item in sublist]\n",
    "    tf.logging.info('----train variables----')\n",
    "    for item in train_var:\n",
    "        tf.logging.info(item)\n",
    "\n",
    "    return train_var\n",
    "\n",
    "def load_layers(min_stack=0, first_min_block=0):\n",
    "    train_scope = []\n",
    "    \n",
    "    min_block = first_min_block\n",
    "    for stack in range(0, min_stack):\n",
    "        for res_block in range(min_block, num_res_blocks):\n",
    "            train_scope.append(\"feature_extractor/{}_{}_one/\".format(stack, res_block))\n",
    "\n",
    "            train_scope.append(\"feature_extractor/{}_{}_two/\".format(stack, res_block))\n",
    "            if stack>0 and res_block == 0:\n",
    "                train_scope.append(\"feature_extractor/{}_{}_three/\".format(stack, res_block))\n",
    "        min_block = 0      \n",
    "\n",
    "    var = []\n",
    "    if min_stack > 0:\n",
    "        var.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'feature_extractor/first'))\n",
    "    for scope in train_scope:\n",
    "        var.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n",
    "    train_var = [item for sublist in var for item in sublist]\n",
    "    for item in train_var:\n",
    "        tf.logging.info(item)\n",
    "\n",
    "    return train_var\n",
    "\n",
    "def trainable_var():\n",
    "    var_names = []    \n",
    "    for var in tf.trainable_variables():    \n",
    "        var_names.append(var.name)\n",
    "        print(var.name)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    cond1 = tf.cond(tf.greater(epoch, 40), lambda:lr*1e-1, lambda:lr)\n",
    "    cond2 = tf.cond(tf.greater(epoch, 60), lambda:lr*1e-2, lambda:cond1)\n",
    "    cond3 = tf.cond(tf.greater(epoch, 80), lambda:lr*1e-3, lambda:cond2)\n",
    "    cond4 = tf.cond(tf.greater(epoch, 90), lambda:lr*0.5e-3, lambda:cond3)\n",
    "    cond4 = tf.identity(cond4, name='cond4')\n",
    "    return cond4\n",
    "\n",
    "\n",
    "def inference(transfer):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    train_dataset = data.train_input_fn(classcount=classcount, use_train=FLAGS.use_train, datapath=cifar10_path,\n",
    "                                        dataset=FLAGS.dataset, batchsize=FLAGS.batchsize, maxepochs=maxepochs)\n",
    "    train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "    val_dataset = data.val_input_fn(classcount=classcount, use_val=use_val, datapath=cifar10_path,\n",
    "                                    dataset=FLAGS.dataset, batchsize=FLAGS.batchsize, maxepochs=maxepochs)\n",
    "    val_iterator = val_dataset.make_initializable_iterator()\n",
    "    #-------------------------------------------------------------------------------------%\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.contrib.data.Iterator.from_string_handle(\n",
    "        handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "    features, labels = iterator.get_next()\n",
    "    \n",
    "    global_step=tf.get_variable('global_step',(), trainable=False, initializer=tf.zeros_initializer)\n",
    "    \n",
    "    epoch = tf.ceil(global_step*FLAGS.batchsize/FLAGS.use_train)\n",
    "    epoch = tf.identity(epoch, name='epoch')\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), classcount)\n",
    "    logits, update_ops = plas_resnet.resnet_model(features, onehot_labels, 'feature_extractor', num_res_blocks, classcount)\n",
    "    logits = tf.identity(logits, 'logits')\n",
    "    #-------------------------------------------------------------------------------------%\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=onehot_labels, logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    lr = lr_schedule(epoch)\n",
    "    lr = tf.identity(lr, name='lr')\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    #--------------------------------------------------------------------------------------#\n",
    "    model_dir = make_model_dir()\n",
    "    \n",
    "    # transfer\n",
    "    if transfer:\n",
    "        train_var = train_layers(FLAGS.from_stack, 0)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=global_step, var_list=train_var)\n",
    "        model_name = model_dir+'/trans_plas_res'+str(6*num_res_blocks+2)\n",
    "    # pretrain\n",
    "    else:\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=global_step)\n",
    "        model_name = model_dir+'/pre_plas_res'+str(6*num_res_blocks+2)    \n",
    "    #-------------------------------------------------------------------------------------%\n",
    "    classes = tf.argmax(input=logits, axis=1)\n",
    "    correct_prediction = tf.equal(tf.cast(classes, tf.uint8), labels)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    acc = tf.identity(acc, name='accuracy_tensor')\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "        if transfer:\n",
    "            restore_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='feature_extractor/')\n",
    "            #restore_var_list.append(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='classifier/hebb')[0])\n",
    "            restore_var_list.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='classifier/eta')[0])\n",
    "            restore_var_list.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='classifier/alpha')[0])\n",
    "            tf.logging.info('----load variables----')\n",
    "            for item in restore_var_list:\n",
    "                tf.logging.info(item)\n",
    "            #var_list = load_layers(FLAGS.from_stack, 0)\n",
    "            restorer = tf.train.Saver(var_list=restore_var_list)\n",
    "            #util.init_from_checkpoint(pretrained_model, {'feature_extractor/':'transfer/'})\n",
    "            restorer.restore(sess, pretrained_model)\n",
    "    #-------------------------------------------------------------------------------------%\n",
    "        train_handle = sess.run(train_iterator.string_handle())\n",
    "        val_handle = sess.run(val_iterator.string_handle())\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                steps = tf.train.global_step(sess, global_step)\n",
    "                xxx = tf.get_default_graph().get_tensor_by_name(\"classifier/featxx:0\")\n",
    "                hebbb = tf.get_default_graph().get_tensor_by_name(\"classifier/hebb:0\")\n",
    "                ops = [train_op, loss, epoch, acc, lr, update_ops, xxx, hebbb]\n",
    "                _, lossvalue, epoch_num, accuracy, learning_rate, _, xxxx, hebbbb = sess.run(ops, feed_dict = {handle: train_handle})\n",
    "                #xxx = tf.get_default_graph().get_tensor_by_name(\"classifier/featxx:0\")\n",
    "                print(xxxx,hebbbb)\n",
    "                if steps % 1 == 0:\n",
    "                    tf.logging.info('epoch:'+str(epoch_num)+' step:'+str(steps)+' learning rate:'+str(learning_rate))\n",
    "                    tf.logging.info('loss:'+str(lossvalue)+' batch accuracy:'+str(accuracy))\n",
    "                if steps % FLAGS.eval_step == 0:\n",
    "                    if not transfer:\n",
    "                        saver.save(sess, model_name, global_step = steps)\n",
    "                    evalutation(sess, val_iterator, correct_prediction, handle, val_handle)\n",
    "                    \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        evalutation(sess, val_iterator, correct_prediction, handle, val_handle)\n",
    "        \n",
    "def evalutation(sess, val_iterator, correct_prediction, handle, val_handle):\n",
    "    correct_sum = 0\n",
    "    count = 0\n",
    "\n",
    "    sess.run(val_iterator.initializer)\n",
    "    while True:\n",
    "        try:                            \n",
    "            correct_prediction_value = sess.run(correct_prediction, feed_dict = {handle: val_handle})\n",
    "            count += correct_prediction_value.shape[0]\n",
    "            correct_sum += np.sum(correct_prediction_value)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    tf.logging.info('eval accuracy:'+str(correct_sum/count))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--save_dir',\n",
    "        type=str,\n",
    "        default='default')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--use_train',\n",
    "        type=int,\n",
    "        default='250')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--mode',\n",
    "        type=str,\n",
    "        default='pretrain')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--from_stack',\n",
    "        type=int,\n",
    "        default='0')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        type=int,\n",
    "        default='1')    \n",
    "\n",
    "    parser.add_argument(\n",
    "        '--eval_step',\n",
    "        type=int,\n",
    "        default='1')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--batchsize',\n",
    "        type=int,\n",
    "        default='16')\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.logging.info('plasticity: True')\n",
    "    tf.logging.info('mode: '+str(FLAGS.mode))\n",
    "    tf.logging.info('traing on dataset: '+str(FLAGS.dataset))\n",
    "    tf.logging.info('use train: '+str(FLAGS.use_train))\n",
    "    tf.logging.info('from stack: '+str(FLAGS.from_stack))\n",
    "    tf.logging.info('model save dir: '+str(FLAGS.save_dir))\n",
    "    tf.logging.info('eval step: '+str(FLAGS.eval_step))\n",
    "    tf.logging.info('batch size: '+str(FLAGS.batchsize))\n",
    "\n",
    "    if FLAGS.mode == 'transfer':\n",
    "        transfer = True\n",
    "    elif FLAGS.mode == 'pretrain':\n",
    "        transfer = False\n",
    "    else:\n",
    "        raise ValueError('unknown mode')\n",
    "\n",
    "    inference(transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
